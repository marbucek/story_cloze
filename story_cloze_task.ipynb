{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads stories from the validation file. Each sentence has two possible outcomes: either the 5th or 6th sentence. The label (1 or 2) indicates which sentence contains the correct ending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/data_val.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vld set contains both true and fake stories, roc set only the true stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def read_data(file, data_type = 'val'):\n",
    "    data = pd.read_csv(file)\n",
    "    stories = []; labels = []\n",
    "    for _, row in data.iterrows():\n",
    "        story = {}\n",
    "        if data_type == 'roc':\n",
    "            story['ctx'] = [nltk.word_tokenize(sentence.lower()) for sentence in list(row[2:6])]\n",
    "            story['ends'] = nltk.word_tokenize(row[6].lower())\n",
    "            stories.append((story,))\n",
    "            labels.append(+1)\n",
    "        if data_type == 'val':\n",
    "            story['ctx'] = [nltk.word_tokenize(sentence.lower()) for sentence in list(row[1:5])]\n",
    "            story['ends'] = [nltk.word_tokenize(sentence.lower()) for sentence in list(row[5:7])]\n",
    "            stories.append(story.copy())\n",
    "            labels.append(row[7])\n",
    "    return stories, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories, labels = read_data('data/data_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "model = models.KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dim_embedding = 300\n",
    "\n",
    "np.random.seed(10)\n",
    "UKN = np.random.uniform(low=-0.25, high=0.25, size=dim_embedding)\n",
    "\n",
    "def w2v(token):\n",
    "    try:\n",
    "        return model[token]\n",
    "    except:\n",
    "        return UKN\n",
    "\n",
    "def centroid(tokens):\n",
    "        if len(tokens) == 0:\n",
    "            return np.zeros(shape = [dim_embedding,])\n",
    "        else:\n",
    "#             for token in tokens:\n",
    "#                 print(token, w2v(token)[0:10])\n",
    "            return np.mean([w2v(token) for token in tokens], axis = 0)\n",
    "    \n",
    "def cosine(vec1, vec2):\n",
    "    if np.sum(vec1**2)*np.sum(vec2**2) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.sum(vec1*vec2)/np.sqrt(np.sum(vec1**2)*np.sum(vec2**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove dots, dashes and punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define features as in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Centroid feature: concatenates the centroid of the context with the centroid of the ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_feature(story):\n",
    "    ctr_context = np.mean([centroid(sentence) for sentence in story['ctx']], axis = 0)\n",
    "    return np.concatenate([ctr_context, centroid(story['end'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average similarity: computes cosine distance between the centroid vector of the ending and vectors of the words in the context. Similarity scores of the words with top_N highest scores are returned as the feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_sim(story, top_N = [1,2,3,5]):\n",
    "    ctx_embeddings = [w2v(token) for token in list(set(sum(story['ctx'],[])))]\n",
    "    words_similarity = sorted([cosine(embedding, centroid(list(set(story['end'])))) for embedding in ctx_embeddings], reverse = True)\n",
    "    top_similarities = np.asarray([words_similarity[id] for id in top_N])\n",
    "    return top_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Max similarity: for each word in the context, chooses the most similar word from the ending and takes the average of all best word pair similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_sim(story):\n",
    "    words_similarity = []\n",
    "    for token in list(set(sum(story['ctx'],[]))):\n",
    "        words_similarity.append(\n",
    "                    np.max([cosine(w2v(token),w2v(token_end)) for token_end in story['end']])\n",
    "                    )\n",
    "    return [np.mean(words_similarity)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all combinations (POS1, POS2) in the context and ending computes centroid similarity between all words of type POS1 in the context and all words of type POS2 in the ending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_sim(story, POS = ['VBZ','VBN','VBP','VBG','VBD','VB','RBS','RBR','RB','POS','NN','NNS','JJS','JJR','JJ']):\n",
    "    POS_context = nltk.pos_tag(sum(story['ctx'],[]))\n",
    "    POS_end = nltk.pos_tag(story['end'])\n",
    "\n",
    "    pos_sim = []\n",
    "    for pos1 in POS:\n",
    "        for pos2 in POS:\n",
    "            ctr_context = centroid([token for token, pos in POS_context if pos == pos1])\n",
    "            ctr_end = centroid([token for token, pos in POS_end if pos == pos2])\n",
    "            pos_sim.append(cosine(ctr_context, ctr_end))\n",
    "\n",
    "    return np.array(pos_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(stories):\n",
    "#     pool = mp.Pool(n_cpus)\n",
    "#     grid = pool.starmap_async(get_grid_r, [(theta, (Delta_x, Delta_y)) for theta in thetas]).get()\n",
    "    \n",
    "    X = []\n",
    "    for item in stories:\n",
    "        story = {}; features = []\n",
    "        for end in item['ends']:\n",
    "            story['ctx'] = item['ctx']\n",
    "            story['end'] = end\n",
    "            X_story = np.concatenate([feature(story) \n",
    "                                       for feature in [centroid_feature, average_sim, max_sim, pos_sim]])\n",
    "            features.append(X_story)\n",
    "        X.append(np.stack(features))\n",
    "    return np.stack(X)\n",
    "\n",
    "def get_y(labels):\n",
    "    y = []\n",
    "    for label in labels:\n",
    "        y.append(np.array([1,0])) if label == 1 else y.append(np.array([0,1]))\n",
    "    return np.stack(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_labels(clf, X):\n",
    "    labels = []\n",
    "    for x in X:\n",
    "        prob_true = clf.predict_proba(x)[:,1]\n",
    "        labels.append(1) if np.argmax(prob_true) == 0 else labels.append(2)\n",
    "    return np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = get_features(stories)\n",
    "y = get_y(labels)\n",
    "stories_tst, labels_tst = read_data('data/data_test.csv')\n",
    "Xts = get_features(stories_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-12f14ee88aaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mlabels_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Accuracy = {accuracy_score(labels_tst, labels_pred)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "lg = LogisticRegression(C=2.5)\n",
    "lg.fit(X.reshape([2*X.shape[0],-1]), y.reshape(-1))\n",
    "labels_pred = predict_labels(lg, Xts)\n",
    "print(f'Accuracy = {accuracy_score(labels_tst, labels_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
